{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.8.3 - Logistic Regression Mathematics  (SoftMax)\n",
    "\n",
    "\n",
    "**MNIST data, hand written numbers classification (1vsALL)**\n",
    "\n",
    "\n",
    "![sig_plot](../../../images/sig_plot.png)\n",
    "![sig_plot](https://github.com/YonatanRA/da-ce-case-study/blob/master/images/sig_plot.png)\n",
    "\n",
    "\n",
    "\n",
    "### Custom Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time                                                 # time handling\n",
    "    \n",
    "import pandas as pd                                         # dataframe\n",
    "import numpy as np                                          # numerical python, linear algebra \n",
    "\n",
    "import matplotlib.pyplot as plt                             # plots\n",
    "\n",
    "from scipy.optimize import minimize                         # minimize, optimize a function\n",
    "\n",
    "import warnings                                             # removing warnings\n",
    "warnings.filterwarnings('ignore')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We write the main model ecuations**:\n",
    "\n",
    "+ Sigmoid function\n",
    "+ Loss function\n",
    "+ Gradient of loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sigmoid Function**\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "where:\n",
    "\n",
    "$z = \\beta_{0} + \\beta_{1}x_{1} + \\beta_{2}x_{2} + \\dots + \\beta_{n}x_{n}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(X, b):                                 # sigmoid function\n",
    "    \n",
    "    return 1.0/(1.0+np.exp(-np.dot(X,b)))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loss Function**\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum_{i=1}^{m} (y_{i} \\log{(\\sigma(z_{i}))} + (1-y_{i})\\log{(1-\\sigma(z_{i}))}) + \\frac{\\lambda}{2m} \\sum_{j=1}^{n} \\beta_{j}^{2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X, b, y, lambda_reg):              # loss function, function to be minimized \n",
    "    \n",
    "    return -(np.sum(np.log(f(X,b)))+np.dot((y-1).T,(np.dot(X,b))))/y.size + lambda_reg/(2.0*y.size)*np.dot(b[1:],b[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gradient of Loss Function**\n",
    "\n",
    "$$\\frac{\\partial}{\\partial \\beta_{j}} J = \\frac{1}{m} \\sum_{i=1}^{m} (\\sigma(z_{i})-y_{i})x_{ij} + \\frac{\\lambda}{m} \\beta_{j}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_loss(X, b, y, lambda_reg):          # loss function gradient \n",
    "    \n",
    "    return (np.dot(X.T,(f(X,b)-y)))/y.size + lambda_reg/(2.0*y.size)*np.concatenate(([0], b[1:])).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Normalize Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X):                   \n",
    "    \n",
    "    '''\n",
    "    This aplies the Central Limit Theorem and makes feature engineering\n",
    "    '''\n",
    "    \n",
    "    X_mean=X.mean(axis=0)           # X mean\n",
    "    X_std=X.std(axis=0)             # X std\n",
    "    X_std[X_std==0]=1.0             # if std=0 then std=1\n",
    "    \n",
    "    X=(X-X_mean)/X_std              # CLT\n",
    "    \n",
    "    X=np.insert(X, 0, 1, axis=1)    # feature engineering [1, f1, f2.., fn...] (improves 10%)\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data matrix dimensions: (42000, 785)\n"
     ]
    }
   ],
   "source": [
    "data=pd.read_csv('data/train_Mnist.csv')        # number pictures\n",
    "print ('Data loaded...')\n",
    "\n",
    "display(data.head())\n",
    "\n",
    "matrix_data=data.values   \n",
    "print ('Data matrix dimensions: {}'.format(matrix_data.shape))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizing a picture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(matrix_data[45,1:].reshape(28,28))  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**One-Hot Encoding for Target Variable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y dimensions: (42000, 10)\n",
      "\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "Y=np.zeros((matrix_data.shape[0],10))   \n",
    "\n",
    "print ('Y dimensions: {}'.format(Y.shape))  \n",
    "print ()\n",
    "\n",
    "for i in range(10):\n",
    "    Y[:,i]=np.where(matrix_data[:,0]==i,1,0)\n",
    "    \n",
    "    \n",
    "print(Y[0:10,:]) # first 10 rows "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Split X features and target variable(y) from data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X original dimensions: (42000, 784)\n"
     ]
    }
   ],
   "source": [
    "labels=matrix_data[:,0]             # label, the number itself, 42000 labels\n",
    "\n",
    "X=matrix_data[:,1:]                # pixel numeric data, every column is a pixel \n",
    "\n",
    "print ('X original dimensions: {}'.format(X.shape)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X new dimensions: (42000, 708)\n"
     ]
    }
   ],
   "source": [
    "X=X[:,X.sum(axis=0)!=0]            # drop columns=0 (CAREFULL - Dimension Changes)\n",
    "\n",
    "print ('X new dimensions: {}'.format(X.shape)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**X Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimensions: X=(30000, 708), Y=(30000, 10)\n",
      "\n",
      "Test data dimensions: X=(12000, 708), Y=(12000, 10)\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train=X[0:30000,:], Y[0:30000,:]        # train data custom\n",
    "X_train_sk=X_train.copy()                          # for sklearn (same data)\n",
    "\n",
    "X_test, Y_test=X[30000:,:], Y[30000:,:]            # test data custom\n",
    "X_test_sk=X_test.copy()                            # for sklearn\n",
    "\n",
    "\n",
    "print ('Train data dimensions: X={}, Y={}'.format(X_train.shape, Y_train.shape))  \n",
    "print ()\n",
    "print ('Test data dimensions: X={}, Y={}'.format(X_test.shape, Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Labels Train-Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels dimensions: (30000,)\n",
      "\n",
      "Test labels dimensions: (12000,)\n"
     ]
    }
   ],
   "source": [
    "labels_train=labels[0:30000]       # labels for train\n",
    "labels_test=labels[30000:]         # labels for test\n",
    "\n",
    "print ('Train labels dimensions: {}'.format(labels_train.shape))\n",
    "print ()\n",
    "print ('Test labels dimensions: {}'.format(labels_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Normalize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data normalized.\n"
     ]
    }
   ],
   "source": [
    "X_train=normalize(X_train)\n",
    "X_test=normalize(X_test)\n",
    "\n",
    "print ('Data normalized.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beta Parameters Initial Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_val=np.random.rand(X_train.shape[1])  # random uniform distributed\n",
    "\n",
    "B_opt=np.zeros((X_train.shape[1],10))         # initial value parameter matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L2 Regularization parameter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_reg=100.           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Optimizing 0 vs all.\n",
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 61, function evaluations: 61, CG iterations: 351, optimality: 9.70e-05, constraint violation: 0.00e+00, execution time:  1.6 s.\n",
      "Done.\n",
      "\n",
      "\n",
      "Optimizing 1 vs all.\n",
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 49, function evaluations: 49, CG iterations: 225, optimality: 6.74e-05, constraint violation: 0.00e+00, execution time:  1.3 s.\n",
      "Done.\n",
      "\n",
      "\n",
      "Optimizing 2 vs all.\n",
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 59, function evaluations: 59, CG iterations: 344, optimality: 7.97e-05, constraint violation: 0.00e+00, execution time:  1.5 s.\n",
      "Done.\n",
      "\n",
      "\n",
      "Optimizing 3 vs all.\n",
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 59, function evaluations: 59, CG iterations: 313, optimality: 8.84e-05, constraint violation: 0.00e+00, execution time:  1.5 s.\n",
      "Done.\n",
      "\n",
      "\n",
      "Optimizing 4 vs all.\n",
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 61, function evaluations: 61, CG iterations: 328, optimality: 8.73e-05, constraint violation: 0.00e+00, execution time:  1.6 s.\n",
      "Done.\n",
      "\n",
      "\n",
      "Optimizing 5 vs all.\n",
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 63, function evaluations: 63, CG iterations: 359, optimality: 6.13e-05, constraint violation: 0.00e+00, execution time:  1.7 s.\n",
      "Done.\n",
      "\n",
      "\n",
      "Optimizing 6 vs all.\n",
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 69, function evaluations: 69, CG iterations: 402, optimality: 9.84e-05, constraint violation: 0.00e+00, execution time:  1.8 s.\n",
      "Done.\n",
      "\n",
      "\n",
      "Optimizing 7 vs all.\n",
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 56, function evaluations: 56, CG iterations: 286, optimality: 9.88e-05, constraint violation: 0.00e+00, execution time:  1.6 s.\n",
      "Done.\n",
      "\n",
      "\n",
      "Optimizing 8 vs all.\n",
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 56, function evaluations: 56, CG iterations: 302, optimality: 9.02e-05, constraint violation: 0.00e+00, execution time:  1.8 s.\n",
      "Done.\n",
      "\n",
      "\n",
      "Optimizing 9 vs all.\n",
      "`gtol` termination condition is satisfied.\n",
      "Number of iterations: 71, function evaluations: 71, CG iterations: 460, optimality: 7.06e-05, constraint violation: 0.00e+00, execution time:  2.3 s.\n",
      "Done.\n",
      "\n",
      "Custom model fit time: 17.02 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "init_opt=time.time()     # starting point\n",
    "\n",
    "\n",
    "for i in range(10):      # 10 labels, 10 numbers             \n",
    "    \n",
    "    print ('\\n\\nOptimizing {} vs all.'.format(i))\n",
    "\n",
    "    def opt_loss(b):                        \n",
    "        return loss(X_train, b, Y_train[:,i], lambda_reg) \n",
    "\n",
    "    def opt_grad_loss(b):                   \n",
    "        return grad_loss(X_train, b, Y_train[:,i], lambda_reg)\n",
    "\n",
    "    \n",
    "    i_opt=time.time() \n",
    "    \n",
    "    model=minimize(opt_loss, \n",
    "                   initial_val, \n",
    "                   method='trust-constr', \n",
    "                   jac=opt_grad_loss, \n",
    "                   tol=1e-4, \n",
    "                   options={'disp':True}) \n",
    "    \n",
    "    print ('Done.')\n",
    "    \n",
    "    B_opt[:,i]=model.x\n",
    "\n",
    "    \n",
    "t_custom=time.time()-init_opt   # time for complete optimization\n",
    "\n",
    "\n",
    "print ('\\nCustom model fit time: {:.2f} seconds.\\n' .format(t_custom)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check model predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=[]                  # predicted labels\n",
    "y_prob=[]                  # probabilities\n",
    "\n",
    "\n",
    "def summary(data):        # testing\n",
    "    for e in data:\n",
    "        \n",
    "        name, label, Xs=e         \n",
    "        label_len=label.size\n",
    "        \n",
    "        probs=np.zeros((label_len,2))      # labels with their probs \n",
    "        count=0                            # success count\n",
    "        \n",
    "        \n",
    "        for sample in range(label_len): \n",
    "            for n in range(10):\n",
    "                \n",
    "                beta=B_opt[:,n]                       # beta parameters\n",
    "                probs[n,0]=n\n",
    "                probs[n,1]=f(Xs[sample,:],beta)       # evaluating the prediction\n",
    "                \n",
    "            probs=probs[probs[:,1].argsort()[::-1]]   # order by probability\n",
    "            y_pred.append(probs[0,0])\n",
    "            y_prob.append(probs[0,1])\n",
    "            \n",
    "            if probs[0,0]==label[sample]:         # if success count + 1\n",
    "                count+=1\n",
    "                \n",
    "        print ('\\n{}'.format(name))\n",
    "        print ('{}/{} ==> {:.4}% accuracy'.format(count, label_len, count/label_len*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results Custom model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training  :\n",
      "27952/30000 ==> 93.17% accuracy\n",
      "\n",
      "Test  :\n",
      "10979/12000 ==> 91.49% accuracy\n"
     ]
    }
   ],
   "source": [
    "summary([('Training  :', labels_train, X_train)])\n",
    "summary([('Test  :', labels_test, X_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saving Beta Parameters, save the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beta parameters dimensions=(709, 10)\n"
     ]
    }
   ],
   "source": [
    "print ('Beta parameters dimensions={}'.format(B_opt.shape))\n",
    "\n",
    "df=pd.DataFrame(B_opt, columns=[i+1 for i in range(B_opt.shape[1])])  \n",
    "\n",
    "#df.to_csv('betas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scikit-Learn Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sklearn model fit time: 3.01 seconds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ini_opt_sk=time.time()\n",
    "\n",
    "logreg=LogisticRegression(C=0.01,         \n",
    "                          penalty='l2', \n",
    "                          tol=0.0001, \n",
    "                          max_iter=70,\n",
    "                          solver='lbfgs', \n",
    "                          multi_class='multinomial')\n",
    "\n",
    "\n",
    "logreg.fit(X_train_sk, labels_train)\n",
    "\n",
    "\n",
    "\n",
    "t_sklearn=time.time()-ini_opt_sk\n",
    "\n",
    "print ('\\nSklearn model fit time: {:.2f} seconds.\\n' .format(t_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_sk(data):\n",
    "    for e in data:\n",
    "        \n",
    "        name, label, Xs=e\n",
    "        label_len=label.size\n",
    "        \n",
    "        y_pred_sk=logreg.predict(Xs)\n",
    "        \n",
    "        count=0\n",
    "        \n",
    "        for sample in range(label_len):\n",
    "            if y_pred_sk[sample]==label[sample]:         \n",
    "                count+=1\n",
    "        \n",
    "        print ('\\n{}'.format(name))\n",
    "        print ('{}/{} ==> {:.4}% accuracy'.format(count, label_len, count/label_len*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training  :\n",
      "28108/30000 ==> 93.69% accuracy\n",
      "\n",
      "Test  :\n",
      "11014/12000 ==> 91.78% accuracy\n"
     ]
    }
   ],
   "source": [
    "summary_sk([('Training  :', labels_train, X_train_sk)])\n",
    "summary_sk([('Test  :', labels_test, X_test_sk)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Time Comparision**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SkLearn is 5.65 times faster for 70 iterations.\n"
     ]
    }
   ],
   "source": [
    "print ('SkLearn is {:.2f} times faster for 70 iterations.'.format(t_custom/t_sklearn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Absolute Difference of Results**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is an absolute prediction difference of 4.84% among both models.\n"
     ]
    }
   ],
   "source": [
    "y_pred_sk=logreg.predict(X_test_sk)\n",
    "\n",
    "comp=[y_pred[30000:][i]==y_pred_sk[i] for i in range(len(y_pred[30000:]))]\n",
    "\n",
    "n_equal=len([e for e in comp if e==False])/len(y_pred_sk)\n",
    "\n",
    "print ('There is an absolute prediction difference of {:.2f}% among both models.'.format(n_equal*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy both Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Custom Training:\n",
      "27952/30000 ==> 93.17% accuracy\n",
      "\n",
      "Custom Test :\n",
      "10979/12000 ==> 91.49% accuracy\n",
      "\n",
      "\n",
      "SkLearn Training:\n",
      "28108/30000 ==> 93.69% accuracy\n",
      "\n",
      "SkLearn Test:\n",
      "11014/12000 ==> 91.78% accuracy\n"
     ]
    }
   ],
   "source": [
    "summary([('Custom Training:', labels_train, X_train)])\n",
    "summary([('Custom Test :', labels_test, X_test)])\n",
    "\n",
    "print ()\n",
    "\n",
    "summary_sk([('SkLearn Training:', labels_train, X_train_sk)])\n",
    "summary_sk([('SkLearn Test:', labels_test, X_test_sk)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "finalized": {
   "timestamp": 1624207214081,
   "trusted": false
  },
  "kernelspec": {
   "display_name": "clase",
   "language": "python",
   "name": "clase"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
